# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14L7qcE2OuQkBGGUJn11FrhBAlYRC6kOG
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
# %matplotlib inline
from google.colab import files
import pandas as pd

# Upload the dataset file
uploaded = files.upload()

# Read the uploaded file into a DataFrame
import io
df = pd.read_csv('LymeDisease_9211_county.csv')

df.head()

df.tail()

df.isnull().sum()

df['ConfirmedCount_1992_1996'] = df['ConfirmedCount_1992_1996'].fillna(df['ConfirmedCount_1992_1996'].median())
df['ConfirmedCount_1997_2001'] = df['ConfirmedCount_1997_2001'].fillna(df['ConfirmedCount_1997_2001'].median())
df['ConfirmedCount_2002_2006'] = df['ConfirmedCount_2002_2006'].fillna(df['ConfirmedCount_2002_2006'].median())
df['ConfirmedCount_2007_2011'] = df['ConfirmedCount_2007_2011'].fillna(df['ConfirmedCount_2007_2011'].median())

df['ConfirmedCount_1992_1996'].isnull().sum()

df.isnull().sum()

df.describe()

df.info()

df.rename(columns={'CountyCode':'CountryCode'},inplace=True)
df.rename(columns={'CountyName':'CountryName'},inplace=True)

df

df['CountryName'].unique()

df['StateName'].unique()

import seaborn as sns
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')
# Set up the matplotlib figure
plt.figure(figsize=(14, 8))

# 1. Distribution of Confirmed Counts for Each Period
plt.subplot(2, 2, 1)
sns.histplot(df['ConfirmedCount_1992_1996'].dropna(), bins=5, kde=True)
plt.title('Distribution of Confirmed Counts (1992-1996)')
plt.xlabel('Confirmed Counts')
plt.ylabel('Frequency')

plt.subplot(2, 2, 2)
sns.histplot(df['ConfirmedCount_1997_2001'].dropna(), bins=5, kde=True)
plt.title('Distribution of Confirmed Counts (1997-2001)')
plt.xlabel('Confirmed Counts')
plt.ylabel('Frequency')

plt.subplot(2, 2, 3)
sns.histplot(df['ConfirmedCount_2002_2006'].dropna(), bins=5, kde=True)
plt.title('Distribution of Confirmed Counts (2002-2006)')
plt.xlabel('Confirmed Counts')
plt.ylabel('Frequency')

plt.subplot(2, 2, 4)
sns.histplot(df['ConfirmedCount_2007_2011'].dropna(), bins=5, kde=True)
plt.title('Distribution of Confirmed Counts (2007-2011)')
plt.xlabel('Confirmed Counts')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

from sklearn.preprocessing import LabelEncoder

df['StateName'] = LabelEncoder().fit_transform(df['StateName'])
df['CountryName'] = LabelEncoder().fit_transform(df['CountryName'])

correlation = df.corr()
sns.heatmap(correlation,annot=True,fmt=".2f")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor

X = df.drop('ConfirmedCount_2007_2011',axis=1)
y = df['ConfirmedCount_2007_2011']

X.head()

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=20)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

xgbModel = XGBRegressor()
xgbModel.fit(X_train_scaled,y_train)

predictions = xgbModel.predict(X_test_scaled)
from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error

print("R2Score",r2_score(y_test,predictions))
print("MSE",mean_squared_error(y_test,predictions))
print("MAE",mean_absolute_error(y_test,predictions))
print("RMSE",np.sqrt(mean_squared_error(y_test,predictions)))

from sklearn.ensemble import GradientBoostingRegressor,AdaBoostRegressor
grandientModel = GradientBoostingRegressor()
grandientModel.fit(X_train_scaled,y_train)

grandientModelPredict = grandientModel.predict(X_test_scaled)
print("R2Score",r2_score(y_test,grandientModelPredict))
print("MSE",mean_squared_error(y_test,grandientModelPredict))
print("MAE",mean_absolute_error(y_test,grandientModelPredict))
print("RMSE",np.sqrt(mean_squared_error(y_test,grandientModelPredict)))

adaboostModel = AdaBoostRegressor()
adaboostModel.fit(X_train_scaled,y_train)

adaboosttModelPredict = adaboostModel.predict(X_test_scaled)
print("R2Score",r2_score(y_test,adaboosttModelPredict))
print("MSE",mean_squared_error(y_test,adaboosttModelPredict))
print("MAE",mean_absolute_error(y_test,adaboosttModelPredict))
print("RMSE",np.sqrt(mean_squared_error(y_test,adaboosttModelPredict)))

from sklearn.model_selection import RandomizedSearchCV

adaboostParams = {
    "n_estimators":[50,60,70,80,90,100],
    "learning_rate":[0.1,0.01,0.001,0.001]
}
gradientParams = {
    "loss":["squared_error","absolute_error","huber","quantile"],
    "criterion":["frideman_mse","squared_error","mse"],
#     "min_sample_split":[2.0,8.0,15.0,20.0],
    "n_estimators":[100,200,500,1000],
    "max_depth":[5,8,15,None,10]

}

def hyperparameterTunning(model,parameter):
    randomAddBoost = RandomizedSearchCV(estimator=model,cv=5,param_distributions=parameter,n_jobs=-1)
    randomAddBoost.fit(X_train_scaled,y_train)
    print("Model---Name",model)
    print("========================================")
    print("BestParameter",randomAddBoost.best_params_)

hyperparameterTunning(adaboostModel,adaboostParams)

adaboostModel = AdaBoostRegressor(n_estimators=50,learning_rate=0.1)
adaboostModel.fit(X_train_scaled,y_train)

adaboostPredict = adaboostModel.predict(X_test_scaled)
print("R2Score",r2_score(y_test,adaboostPredict))
print("MSE",mean_squared_error(y_test,adaboostPredict))
print("MAE",mean_absolute_error(y_test,adaboostPredict))
print("RMSE",np.sqrt(mean_squared_error(y_test,adaboostPredict)))

hyperparameterTunning(grandientModel,gradientParams)

gradienModel = GradientBoostingRegressor(n_estimators=1000,max_depth=8,loss="huber",criterion= 'squared_error')
gradienModel.fit(X_train_scaled,y_train)

from xgboost import XGBRegressor

xgbModel = XGBRegressor()
xgbModel.fit(X_train_scaled,y_train)
predict = xgbModel.predict(X_test_scaled)

print("R2Score",r2_score(y_test,predict))
print("MSE",mean_squared_error(y_test,predict))
print("MAE",mean_absolute_error(y_test,predict))
print("RMSE",np.sqrt(mean_squared_error(y_test,predict)))

xgbParams = {
    "learning_rate":[0.1,0.01],
    "max_depth":[5,8,12,20,30],
    "n_estimators":[100,200,300],
    "colsample_bytree":[0.5,0.8,1,0.3,0.4]
}

hyperparameterTunning(xgbModel,xgbParams)

xgbModelHyper = XGBRegressor(n_estimators=200, max_depth= 5, learning_rate=0.1,colsample_bytree=0.4)
xgbModelHyper.fit(X_train_scaled,y_train)

xgbpredict = xgbModelHyper.predict(X_test_scaled)

print("R2Score",r2_score(y_test,xgbpredict))
print("MSE",mean_squared_error(y_test,xgbpredict))
print("MAE",mean_absolute_error(y_test,xgbpredict))
print("RMSE",np.sqrt(mean_squared_error(y_test,xgbpredict)))

def modelPredicitions(model_name:str,model,X_train,y_train,X_test,y_test):
    model.fit(X_train,y_train)
    ypredict = model.predict(X_test)
    print(f"____MODEL_NAME___{model_name}")
    print("R2Score",r2_score(y_test,xgbpredict))
    print("MSE",mean_squared_error(y_test,xgbpredict))
    print("MAE",mean_absolute_error(y_test,xgbpredict))
    print("RMSE",np.sqrt(mean_squared_error(y_test,xgbpredict)))

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.linear_model import LogisticRegression
linearModel = LinearRegression()
lassoModel = Lasso()
ridgeModel = Ridge()
logReModel = LogisticRegression()
modelPredicitions("LinearRegression",linearModel,X_train_scaled,y_train,X_test_scaled,y_test)
modelPredicitions("LassoRegression",lassoModel,X_train_scaled,y_train,X_test_scaled,y_test)
modelPredicitions("RidgeRegression",ridgeModel,X_train_scaled,y_train,X_test_scaled,y_test)
modelPredicitions("LogisticRegression",logReModel,X_train_scaled,y_train,X_test_scaled,y_test)

